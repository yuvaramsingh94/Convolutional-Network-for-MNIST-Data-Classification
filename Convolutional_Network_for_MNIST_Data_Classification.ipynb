{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network \n",
    "This is a framework for understanding the working of CNN network using the MNIST data\n",
    "thanks ot Udacity Team for providing materials which made me understang the basics of DEEP learning and CNN networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the needed modules to run this program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### I had problem in executing this line\n",
    "#mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "#the above line will import the data in shape of (:,28,28,1)\n",
    "#unfortunately i was not able to run this code in my system\n",
    "#so i was forced to use this code , which gives the image data of shape (:,784)\n",
    "#mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "#this is the reason of converting the data shape . if you are able to import the image data of shape(28,28,1)\n",
    "#then you dont have to run the conversion block of this program\n",
    "###else , yu have to run those block\n",
    "\n",
    "\n",
    "#mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage place\n",
    "create a place to store the weights and bias after running the training module . this data can be used to classify images of the MNIST data later without having to re-run the training again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a location to save the data\n",
    "save  = 'D:\\\\tutorials\\\\nanodegree\\\\SDC\\\\unit_5_CNN\\\\code\\\\saved_data\\\\convModel.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img = mnist.train.images\n",
    "labels = mnist.train.labels\n",
    "validImg = mnist.validation.images\n",
    "validLables = mnist.validation.labels\n",
    "testImg = mnist.test.images\n",
    "testLabels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "n_classes = 10\n",
    "epochs = 10\n",
    "batch_size = 128 # change this if you have a good system (256, 512 ,1024 ,...)\n",
    "test_valid_size = 10\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the shape of the train , test and validation data\n",
    "convert the image data of shape (:,784) to (:,28,28,1) . this is the shape on which we are going to apply our CNN \n",
    "\n",
    "you dont have to do this if you are using \n",
    "\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the data is converted \n",
      "the new shapes are :\n",
      "Train image (55000, 28, 28, 1)\n",
      "Test image (10000, 28, 28, 1)\n",
      "Validation image (5000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "if img.shape[1] == 784:\n",
    "    img = np.reshape(img,(-1,28,28,1))\n",
    "    labels = np.reshape(labels,(-1,10))\n",
    "    validImg = np.reshape(validImg,(-1,28,28,1))\n",
    "    validLables = np.reshape(validLables,(-1,10))\n",
    "    testImg = np.reshape(testImg,(-1,28,28,1))\n",
    "    testLabels = np.reshape(testLabels,(-1,10))\n",
    "    #print (labels.shape)\n",
    "    print ('all the data is converted ')\n",
    "    print ('the new shapes are :')\n",
    "    print ('Train image {}'.format(img.shape))\n",
    "    print ('Test image {}'.format(testImg.shape))\n",
    "    print ('Validation image {}'.format(validImg.shape))\n",
    "else:\n",
    "    print (' you dont have to convert it .. your data looks fine : ) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of parameters\n",
    "these are the parameters which we are going to use . feel free to change and play around with it\n",
    "\n",
    "in my case , my laptop is not having a GPU build in and my RAM is just 6GB . i have restricted myself to \n",
    "batch_size of 128 . if you are running this on a higher config system , feel freeto change it (256,512,1024.....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "n_classes = 10\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "test_valid_size = 10\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "no_of_data = 100# change this number to have more or less testing and validation data\n",
    "valid_x = validImg[0:no_of_data,:,:,:]\n",
    "valid_y = validLables[0:no_of_data,:]\n",
    "test_x  = testImg[0:no_of_data,:,:,:]\n",
    "test_y  = testLabels[0:no_of_data,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weights and biase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weight = {'wc1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "          'wc2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "          'wd1':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "          'out':tf.Variable(tf.random_normal([1024,n_classes]))}\n",
    "\n",
    "biases = {'bc1':tf.Variable(tf.random_normal([32])),\n",
    "          'bc2':tf.Variable(tf.random_normal([64])),\n",
    "          'bd1':tf.Variable(tf.random_normal([1024])),\n",
    "          'out':tf.Variable(tf.random_normal([n_classes]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a saver to save all our work\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begining of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create convolute layer\n",
    "def conv2d (x,w,b,stride = 1):\n",
    "    x = tf.nn.conv2d(x,w,strides=[1,stride,stride,1],padding='SAME')\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply max pooling to the convolute layer\n",
    "def maxpool2d(x,k):\n",
    "    return tf.nn.max_pool(x,ksize = [1,k,k,1],strides = [1,k,k,1],padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# creation of the Graph\n",
    "'''\n",
    "Layers of our Graph\n",
    "Input layer\n",
    "convolution layer 1\n",
    "convolution layer 2\n",
    "fully connected layer \n",
    "output layer\n",
    "'''\n",
    "def conv_net(x,weight,biases,dropout):\n",
    "    convnetLayer1 = conv2d(x,weight['wc1'],biases['bc1'])\n",
    "    convnetLayer1 = maxpool2d(convnetLayer1,k=2)\n",
    "    \n",
    "    convnetLayer2 = conv2d(convnetLayer1,weight['wc2'],biases['bc2'])\n",
    "    convnetLayer2 = maxpool2d(convnetLayer2,k=2)\n",
    "    \n",
    "    fc1 = tf.reshape(convnetLayer2, [-1, weight['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weight['wd1']), biases['bd1'])\n",
    "    \n",
    "    \n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1,dropout)\n",
    "    out = tf.add(tf.matmul(fc1,weight['out']),biases['out'])\n",
    "    \n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder creation\n",
    "create placeholder for \n",
    "* input image\n",
    "* Lables\n",
    "* Logits\n",
    "* cost\n",
    "* optimizer\n",
    "* accuracy\n",
    "* predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "logits = conv_net(x,weight,biases,keep_prob)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits ,\n",
    "                                                             labels = y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\\\n",
    "                .minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute our graph\n",
    "Time to check our creation . run the block below to start training process and store the session onto a file specified\n",
    "\n",
    "#### Time to complete \n",
    " it took me over 2 hours to complete the process of training . so , be prepered to burn your waiting time while our network is training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 1 is initialized\n",
      "0 epoch\n",
      "the process is at 0 epoch , 0 batch\n",
      "the process is at 0 epoch , 100 batch\n",
      "the process is at 0 epoch , 200 batch\n",
      "the process is at 0 epoch , 300 batch\n",
      "the process is at 0 epoch , 400 batch\n",
      "time taken for execution of epoch 0 is 584.0991532802582\n",
      "the loss in this calculation is : 669.2898559570312\n",
      "Epoch  1, Batch 429 -Loss:   669.2899 Validation Accuracy: 0.760000\n",
      "1 epoch\n",
      "the process is at 1 epoch , 0 batch\n",
      "the process is at 1 epoch , 100 batch\n",
      "the process is at 1 epoch , 200 batch\n",
      "the process is at 1 epoch , 300 batch\n",
      "the process is at 1 epoch , 400 batch\n",
      "time taken for execution of epoch 1 is 2222.907201528549\n",
      "the loss in this calculation is : 361.03076171875\n",
      "Epoch  2, Batch 429 -Loss:   361.0308 Validation Accuracy: 0.810000\n",
      "2 epoch\n",
      "the process is at 2 epoch , 0 batch\n",
      "the process is at 2 epoch , 100 batch\n",
      "the process is at 2 epoch , 200 batch\n",
      "the process is at 2 epoch , 300 batch\n",
      "the process is at 2 epoch , 400 batch\n",
      "time taken for execution of epoch 2 is 3078.3980753421783\n",
      "the loss in this calculation is : 316.85040283203125\n",
      "Epoch  3, Batch 429 -Loss:   316.8504 Validation Accuracy: 0.810000\n",
      "3 epoch\n",
      "the process is at 3 epoch , 0 batch\n",
      "the process is at 3 epoch , 100 batch\n",
      "the process is at 3 epoch , 200 batch\n",
      "the process is at 3 epoch , 300 batch\n",
      "the process is at 3 epoch , 400 batch\n",
      "time taken for execution of epoch 3 is 3817.4735383987427\n",
      "the loss in this calculation is : 262.2535400390625\n",
      "Epoch  4, Batch 429 -Loss:   262.2535 Validation Accuracy: 0.820000\n",
      "4 epoch\n",
      "the process is at 4 epoch , 0 batch\n",
      "the process is at 4 epoch , 100 batch\n",
      "the process is at 4 epoch , 200 batch\n",
      "the process is at 4 epoch , 300 batch\n",
      "the process is at 4 epoch , 400 batch\n",
      "time taken for execution of epoch 4 is 4453.077242136002\n",
      "the loss in this calculation is : 235.44314575195312\n",
      "Epoch  5, Batch 429 -Loss:   235.4431 Validation Accuracy: 0.790000\n",
      "5 epoch\n",
      "the process is at 5 epoch , 0 batch\n",
      "the process is at 5 epoch , 100 batch\n",
      "the process is at 5 epoch , 200 batch\n",
      "the process is at 5 epoch , 300 batch\n",
      "the process is at 5 epoch , 400 batch\n",
      "time taken for execution of epoch 5 is 5219.512498378754\n",
      "the loss in this calculation is : 201.47671508789062\n",
      "Epoch  6, Batch 429 -Loss:   201.4767 Validation Accuracy: 0.830000\n",
      "6 epoch\n",
      "the process is at 6 epoch , 0 batch\n",
      "the process is at 6 epoch , 100 batch\n",
      "the process is at 6 epoch , 200 batch\n",
      "the process is at 6 epoch , 300 batch\n",
      "the process is at 6 epoch , 400 batch\n",
      "time taken for execution of epoch 6 is 5860.239255666733\n",
      "the loss in this calculation is : 192.14382934570312\n",
      "Epoch  7, Batch 429 -Loss:   192.1438 Validation Accuracy: 0.780000\n",
      "7 epoch\n",
      "the process is at 7 epoch , 0 batch\n",
      "the process is at 7 epoch , 100 batch\n",
      "the process is at 7 epoch , 200 batch\n",
      "the process is at 7 epoch , 300 batch\n",
      "the process is at 7 epoch , 400 batch\n",
      "time taken for execution of epoch 7 is 6482.382887601852\n",
      "the loss in this calculation is : 179.4925537109375\n",
      "Epoch  8, Batch 429 -Loss:   179.4926 Validation Accuracy: 0.800000\n",
      "8 epoch\n",
      "the process is at 8 epoch , 0 batch\n",
      "the process is at 8 epoch , 100 batch\n",
      "the process is at 8 epoch , 200 batch\n",
      "the process is at 8 epoch , 300 batch\n",
      "the process is at 8 epoch , 400 batch\n",
      "time taken for execution of epoch 8 is 7103.25749707222\n",
      "the loss in this calculation is : 186.35693359375\n",
      "Epoch  9, Batch 429 -Loss:   186.3569 Validation Accuracy: 0.770000\n",
      "9 epoch\n",
      "the process is at 9 epoch , 0 batch\n",
      "the process is at 9 epoch , 100 batch\n",
      "the process is at 9 epoch , 200 batch\n",
      "the process is at 9 epoch , 300 batch\n",
      "the process is at 9 epoch , 400 batch\n",
      "time taken for execution of epoch 9 is 7785.140857934952\n",
      "the loss in this calculation is : 163.1596221923828\n",
      "Epoch 10, Batch 429 -Loss:   163.1596 Validation Accuracy: 0.770000\n",
      "Testing Accuracy: 0.8799999952316284\n",
      "total time of execusion : 7785.77493929863\n",
      "the process is saved correctly !!!\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print ('stage 1 is initialized')\n",
    "    for epoch in range(epochs):\n",
    "            print ('{} epoch'.format(epoch))\n",
    "            for batch in range(int(img.shape[0]/batch_size)):\n",
    "                batch_x = img[0+(batch*batch_size):(batch*batch_size)+batch_size,:,:,:]\n",
    "                batch_y = labels[0+(batch*batch_size):(batch*batch_size)+batch_size,:]\n",
    "                \n",
    "                \n",
    "                sess.run(optimizer,feed_dict={x:batch_x,y:batch_y,keep_prob : dropout})\n",
    "            \n",
    "                loss=sess.run(cost,feed_dict={x:batch_x,y:batch_y,keep_prob : 1.0})\n",
    "                if batch%100 ==0:\n",
    "                    print ('the process is at {} epoch , {} batch'.format(epoch,batch))\n",
    "                \n",
    "            print ('time taken for execution of epoch {} is {}'.format(epoch,time.time()-start_time))\n",
    "            print ('the loss in this calculation is : {}'.format(loss))\n",
    "            \n",
    "            # to test the validation of our model\n",
    "            valid_acc = sess.run(accuracy,feed_dict={\n",
    "                x: valid_x,\n",
    "                y: valid_y,\n",
    "                keep_prob: 1.})\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "    # to print the total accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: test_x,\n",
    "        y:test_y,\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))\n",
    "    print ('total time of execusion : {}'.format(time.time()-start_time))\n",
    "    saver.save(sess,save)\n",
    "    print('the process is saved correctly !!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finished \n",
    "hope this code worked without giving you hard time . \n",
    "my results were \n",
    "Epoch 10, Batch 429 -Loss:   163.1596 Validation Accuracy: 0.770000\n",
    "Testing Accuracy: 0.8799999952316284\n",
    "total time of execusion : 7785.77493929863\n",
    "\n",
    "##### I like to see your model bet mine  . thanks !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
